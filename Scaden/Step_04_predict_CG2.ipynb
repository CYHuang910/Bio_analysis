{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Github 找到有人建過 pytorch 版的 Scaden\n",
    "###### https://github.com/poseidonchan/TAPE/blob/main/Experiments/pytorch_scaden_PBMConly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  9 22:08:34 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "|  0%   30C    P8     9W / 280W |  10941MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:AF:00.0 Off |                  N/A |\r\n",
      "|  0%   35C    P8    16W / 280W |   3471MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import anndata\n",
    "from anndata import read_h5ad\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from torchsummary import summary\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = read_h5ad('./0902_CGL2_SCT.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 14 × 1971\n",
       "    obs: 'Scaden results', 'CD8+/CD45RA+ Naive Cytotoxic T Cells', 'CD14_Monocytes', 'CD4+/CD45RA+/CD25-Naive T cells', 'CD4+/CD45RO+ Memory T Cells', 'CD56+ Natural Killer Cells', 'CD4+/CD25+ Regulatory T Cells_and_CD4+ T Helper Cells', 'CD19_B_Cells', 'CD8+ Cytotoxic T cells'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = read_h5ad('./0902_CGL2_harmony.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1971)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.10010195, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.18947189, 0.3992759 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.56196386, 0.0631573 , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(adata.X.shape)\n",
    "adata.X[0,0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_no_dropout(nn.Module):\n",
    "    def __init__(self, gene_size, unit):\n",
    "        super(MLP_no_dropout, self).__init__()\n",
    "        self.gene_size = gene_size\n",
    "        self.unit = unit\n",
    "        \n",
    "        self.D1 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.D2 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.D3 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        out1 = self.D1(inputs)\n",
    "        out2 = self.D2(inputs)\n",
    "        out3 = self.D3(inputs)\n",
    "        \n",
    "        out = (out1+out2+out3)/3\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_batch(nn.Module):\n",
    "    def __init__(self, gene_size, unit):\n",
    "        super(MLP_batch, self).__init__()\n",
    "        self.gene_size = gene_size\n",
    "        self.unit = unit\n",
    "        \n",
    "        self.D1 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.D2 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.D3 = nn.Sequential(\n",
    "            nn.Linear(self.gene_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.unit),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        out1 = self.D1(inputs)\n",
    "        out2 = self.D2(inputs)\n",
    "        out3 = self.D3(inputs)\n",
    "        \n",
    "        out = (out1+out2+out3)/3\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess train, validation\n",
    "\n",
    "### Training harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"0902\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = \"no_dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = adata\n",
    "\n",
    "#將 train_data 分開\n",
    "#sample x gene\n",
    "v_exp = val_data.X\n",
    "#v_obs = val_data.obs\n",
    "test = torch.tensor(v_exp, dtype=torch.float32)\n",
    "#test_label = torch.tensor(np.array(v_obs), dtype=torch.float32)\n",
    "celltype = ['CD141+DC','CD4/CD8-C1-CCR7','CD4/CD8-C2-MKI67','CD8-C7-KLRD1','CD8-C9-SLC4A10','Central memory T cells',\n",
    "            'Circulating NK','Conventional dendritic cells(CD1C DC)','Cytotoxicity CD8T','DC-C4-LAMP3',\n",
    "            'Effector memory T cells','Exhausted CD8+ T (Tex) cells','ILCs','Liver-resident NK (lrNK) cell',\n",
    "            'Lymphoid-B','M-C4-GPX3','M1','Mast','Mono','Myeloid-derived suppressor cells','NK','TAM-like',\n",
    "            'Th0','Th1','Treg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './2.model/0902_model/0902_SCT_combine_model_no_dropout.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_no_dropout(v_exp.shape[1], len(celltype)).to(device)\n",
    "mlp.load_state_dict(torch.load(PATH))\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=0.0001)\n",
    "\n",
    "test_dataset = Data.TensorDataset(test)\n",
    "test_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "#print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"0902\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = \"batch_nor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = adata\n",
    "\n",
    "#將 train_data 分開\n",
    "#sample x gene\n",
    "v_exp = val_data.X\n",
    "#v_obs = val_data.obs\n",
    "test = torch.tensor(v_exp, dtype=torch.float32)\n",
    "#test_label = torch.tensor(np.array(v_obs), dtype=torch.float32)\n",
    "celltype = ['CD141+DC','CD4/CD8-C1-CCR7','CD4/CD8-C2-MKI67','CD8-C7-KLRD1','CD8-C9-SLC4A10','Central memory T cells',\n",
    "            'Circulating NK','Conventional dendritic cells(CD1C DC)','Cytotoxicity CD8T','DC-C4-LAMP3',\n",
    "            'Effector memory T cells','Exhausted CD8+ T (Tex) cells','ILCs','Liver-resident NK (lrNK) cell',\n",
    "            'Lymphoid-B','M-C4-GPX3','M1','Mast','Mono','Myeloid-derived suppressor cells','NK','TAM-like',\n",
    "            'Th0','Th1','Treg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './2.model/0902_model/0902_harmony_combine_model_batchnor_before.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_batch(v_exp.shape[1], len(celltype)).to(device)\n",
    "mlp.load_state_dict(torch.load(PATH))\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=0.0001)\n",
    "\n",
    "test_dataset = Data.TensorDataset(test)\n",
    "test_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "#print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine three submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "t_loss =[]\n",
    "val_loss_list = []\n",
    "v_loss = []\n",
    "\n",
    "cor = []\n",
    "cor_list = []\n",
    "val_cor = []\n",
    "val_cor_list = []\n",
    "\n",
    "t_cor_list = []\n",
    "v_cor_list = []\n",
    "t_big_cor_list = []\n",
    "v_big_cor_list = []\n",
    "\n",
    "epochs = 100\n",
    "big_type = [1,3,4,5,6,7,8,10,11,13,14,19,20,21,22,24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch: 1            \n",
      "      epoch: 2            \n",
      "      epoch: 3            \n",
      "      epoch: 4            \n",
      "      epoch: 5            \n",
      "      epoch: 6            \n",
      "      epoch: 7            \n",
      "      epoch: 8            \n",
      "      epoch: 9            \n",
      "      epoch: 10            \n",
      "      epoch: 11            \n",
      "      epoch: 12            \n",
      "      epoch: 13            \n",
      "      epoch: 14            \n",
      "      epoch: 15            \n",
      "      epoch: 16            \n",
      "      epoch: 17            \n",
      "      epoch: 18            \n",
      "      epoch: 19            \n",
      "      epoch: 20            \n",
      "      epoch: 21            \n",
      "      epoch: 22            \n",
      "      epoch: 23            \n",
      "      epoch: 24            \n",
      "      epoch: 25            \n",
      "      epoch: 26            \n",
      "      epoch: 27            \n",
      "      epoch: 28            \n",
      "      epoch: 29            \n",
      "      epoch: 30            \n",
      "      epoch: 31            \n",
      "      epoch: 32            \n",
      "      epoch: 33            \n",
      "      epoch: 34            \n",
      "      epoch: 35            \n",
      "      epoch: 36            \n",
      "      epoch: 37            \n",
      "      epoch: 38            \n",
      "      epoch: 39            \n",
      "      epoch: 40            \n",
      "      epoch: 41            \n",
      "      epoch: 42            \n",
      "      epoch: 43            \n",
      "      epoch: 44            \n",
      "      epoch: 45            \n",
      "      epoch: 46            \n",
      "      epoch: 47            \n",
      "      epoch: 48            \n",
      "      epoch: 49            \n",
      "      epoch: 50            \n",
      "      epoch: 51            \n",
      "      epoch: 52            \n",
      "      epoch: 53            \n",
      "      epoch: 54            \n",
      "      epoch: 55            \n",
      "      epoch: 56            \n",
      "      epoch: 57            \n",
      "      epoch: 58            \n",
      "      epoch: 59            \n",
      "      epoch: 60            \n",
      "      epoch: 61            \n",
      "      epoch: 62            \n",
      "      epoch: 63            \n",
      "      epoch: 64            \n",
      "      epoch: 65            \n",
      "      epoch: 66            \n",
      "      epoch: 67            \n",
      "      epoch: 68            \n",
      "      epoch: 69            \n",
      "      epoch: 70            \n",
      "      epoch: 71            \n",
      "      epoch: 72            \n",
      "      epoch: 73            \n",
      "      epoch: 74            \n",
      "      epoch: 75            \n",
      "      epoch: 76            \n",
      "      epoch: 77            \n",
      "      epoch: 78            \n",
      "      epoch: 79            \n",
      "      epoch: 80            \n",
      "      epoch: 81            \n",
      "      epoch: 82            \n",
      "      epoch: 83            \n",
      "      epoch: 84            \n",
      "      epoch: 85            \n",
      "      epoch: 86            \n",
      "      epoch: 87            \n",
      "      epoch: 88            \n",
      "      epoch: 89            \n",
      "      epoch: 90            \n",
      "      epoch: 91            \n",
      "      epoch: 92            \n",
      "      epoch: 93            \n",
      "      epoch: 94            \n",
      "      epoch: 95            \n",
      "      epoch: 96            \n",
      "      epoch: 97            \n",
      "      epoch: 98            \n",
      "      epoch: 99            \n",
      "      epoch: 100            \n",
      " ----- Finish!! ----- ---\r"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "for inte in range(epochs):\n",
    "    print(\"--- Start training! ---\", end=\"\\r\")\n",
    "    print('      epoch: {}            '.format(inte+1))\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    t_big = 0\n",
    "    v_big = 0\n",
    "    \n",
    "#Validation    \n",
    "    print(\"--- Now evaluation!!! ---\",end=\"\\r\")\n",
    "    mlp.eval()\n",
    "    \n",
    "    # Tell torch not to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for n, D in enumerate(test_dataset):\n",
    "            pred = mlp(D[0].float().cuda())\n",
    "            #VAL = loss_fn(pred, D[1].float().cuda())\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "print(\" ----- Finish!! ----- \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"combine_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.656015</td>\n",
       "      <td>6.453666</td>\n",
       "      <td>0.768478</td>\n",
       "      <td>4.208766</td>\n",
       "      <td>1.711249</td>\n",
       "      <td>23.937073</td>\n",
       "      <td>1.410413</td>\n",
       "      <td>3.489094</td>\n",
       "      <td>9.807796</td>\n",
       "      <td>0.927695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669512</td>\n",
       "      <td>4.619187</td>\n",
       "      <td>1.381890</td>\n",
       "      <td>1.947082</td>\n",
       "      <td>4.356538</td>\n",
       "      <td>0.539389</td>\n",
       "      <td>4.087608</td>\n",
       "      <td>3.165340</td>\n",
       "      <td>0.439703</td>\n",
       "      <td>2.732345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672803</td>\n",
       "      <td>6.568323</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>4.170029</td>\n",
       "      <td>1.731601</td>\n",
       "      <td>23.595064</td>\n",
       "      <td>1.468344</td>\n",
       "      <td>3.591688</td>\n",
       "      <td>9.417401</td>\n",
       "      <td>0.964153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687672</td>\n",
       "      <td>4.599662</td>\n",
       "      <td>1.422911</td>\n",
       "      <td>1.959408</td>\n",
       "      <td>4.461899</td>\n",
       "      <td>0.564665</td>\n",
       "      <td>4.121840</td>\n",
       "      <td>3.191759</td>\n",
       "      <td>0.462567</td>\n",
       "      <td>2.874877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.642758</td>\n",
       "      <td>6.388806</td>\n",
       "      <td>0.756561</td>\n",
       "      <td>4.143879</td>\n",
       "      <td>1.694888</td>\n",
       "      <td>24.085365</td>\n",
       "      <td>1.331089</td>\n",
       "      <td>3.462164</td>\n",
       "      <td>10.115726</td>\n",
       "      <td>0.890888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651254</td>\n",
       "      <td>4.589000</td>\n",
       "      <td>1.345221</td>\n",
       "      <td>1.900900</td>\n",
       "      <td>4.380582</td>\n",
       "      <td>0.492553</td>\n",
       "      <td>4.166725</td>\n",
       "      <td>3.182740</td>\n",
       "      <td>0.422401</td>\n",
       "      <td>2.609814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662778</td>\n",
       "      <td>6.432861</td>\n",
       "      <td>0.772553</td>\n",
       "      <td>4.125156</td>\n",
       "      <td>1.721803</td>\n",
       "      <td>23.966825</td>\n",
       "      <td>1.420724</td>\n",
       "      <td>3.472657</td>\n",
       "      <td>9.901963</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676007</td>\n",
       "      <td>4.566734</td>\n",
       "      <td>1.372900</td>\n",
       "      <td>1.924317</td>\n",
       "      <td>4.390543</td>\n",
       "      <td>0.527891</td>\n",
       "      <td>4.145574</td>\n",
       "      <td>3.189193</td>\n",
       "      <td>0.445519</td>\n",
       "      <td>2.787483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638319</td>\n",
       "      <td>6.400237</td>\n",
       "      <td>0.753536</td>\n",
       "      <td>4.225998</td>\n",
       "      <td>1.682347</td>\n",
       "      <td>24.098427</td>\n",
       "      <td>1.318871</td>\n",
       "      <td>3.366527</td>\n",
       "      <td>10.279809</td>\n",
       "      <td>0.868069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645860</td>\n",
       "      <td>4.581454</td>\n",
       "      <td>1.287504</td>\n",
       "      <td>1.909834</td>\n",
       "      <td>4.285573</td>\n",
       "      <td>0.501148</td>\n",
       "      <td>4.051194</td>\n",
       "      <td>3.168123</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>2.543793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.654502</td>\n",
       "      <td>6.540128</td>\n",
       "      <td>0.766733</td>\n",
       "      <td>4.363331</td>\n",
       "      <td>1.703228</td>\n",
       "      <td>23.737345</td>\n",
       "      <td>1.421851</td>\n",
       "      <td>3.334310</td>\n",
       "      <td>9.805918</td>\n",
       "      <td>0.912278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666816</td>\n",
       "      <td>4.659933</td>\n",
       "      <td>1.319691</td>\n",
       "      <td>1.953964</td>\n",
       "      <td>4.238883</td>\n",
       "      <td>0.563509</td>\n",
       "      <td>3.934856</td>\n",
       "      <td>3.190670</td>\n",
       "      <td>0.439919</td>\n",
       "      <td>2.725107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.635506</td>\n",
       "      <td>6.352584</td>\n",
       "      <td>0.752524</td>\n",
       "      <td>4.157109</td>\n",
       "      <td>1.682888</td>\n",
       "      <td>24.222301</td>\n",
       "      <td>1.312239</td>\n",
       "      <td>3.450688</td>\n",
       "      <td>10.193728</td>\n",
       "      <td>0.869164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644318</td>\n",
       "      <td>4.544249</td>\n",
       "      <td>1.284395</td>\n",
       "      <td>1.890880</td>\n",
       "      <td>4.366358</td>\n",
       "      <td>0.496668</td>\n",
       "      <td>4.098042</td>\n",
       "      <td>3.160381</td>\n",
       "      <td>0.415725</td>\n",
       "      <td>2.561428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.641125</td>\n",
       "      <td>6.410493</td>\n",
       "      <td>0.755701</td>\n",
       "      <td>4.136579</td>\n",
       "      <td>1.688721</td>\n",
       "      <td>24.099165</td>\n",
       "      <td>1.318618</td>\n",
       "      <td>3.484531</td>\n",
       "      <td>10.074389</td>\n",
       "      <td>0.886544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648696</td>\n",
       "      <td>4.583266</td>\n",
       "      <td>1.338185</td>\n",
       "      <td>1.896849</td>\n",
       "      <td>4.400714</td>\n",
       "      <td>0.490130</td>\n",
       "      <td>4.172370</td>\n",
       "      <td>3.179897</td>\n",
       "      <td>0.420714</td>\n",
       "      <td>2.583388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.654554</td>\n",
       "      <td>6.413641</td>\n",
       "      <td>0.764487</td>\n",
       "      <td>4.146720</td>\n",
       "      <td>1.715249</td>\n",
       "      <td>24.026310</td>\n",
       "      <td>1.400277</td>\n",
       "      <td>3.398870</td>\n",
       "      <td>9.919557</td>\n",
       "      <td>0.914965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665617</td>\n",
       "      <td>4.563376</td>\n",
       "      <td>1.357731</td>\n",
       "      <td>1.911407</td>\n",
       "      <td>4.375976</td>\n",
       "      <td>0.519795</td>\n",
       "      <td>4.118711</td>\n",
       "      <td>3.223119</td>\n",
       "      <td>0.437713</td>\n",
       "      <td>2.732165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.655097</td>\n",
       "      <td>6.463675</td>\n",
       "      <td>0.765322</td>\n",
       "      <td>4.193246</td>\n",
       "      <td>1.714998</td>\n",
       "      <td>23.924068</td>\n",
       "      <td>1.404198</td>\n",
       "      <td>3.403003</td>\n",
       "      <td>9.849689</td>\n",
       "      <td>0.911486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>4.581716</td>\n",
       "      <td>1.336415</td>\n",
       "      <td>1.914054</td>\n",
       "      <td>4.369750</td>\n",
       "      <td>0.529535</td>\n",
       "      <td>4.066313</td>\n",
       "      <td>3.221978</td>\n",
       "      <td>0.438410</td>\n",
       "      <td>2.736626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.676916</td>\n",
       "      <td>6.612848</td>\n",
       "      <td>0.788424</td>\n",
       "      <td>4.206869</td>\n",
       "      <td>1.751563</td>\n",
       "      <td>23.389954</td>\n",
       "      <td>1.535797</td>\n",
       "      <td>3.658931</td>\n",
       "      <td>9.125455</td>\n",
       "      <td>0.985704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695497</td>\n",
       "      <td>4.562437</td>\n",
       "      <td>1.407352</td>\n",
       "      <td>1.943234</td>\n",
       "      <td>4.512201</td>\n",
       "      <td>0.609946</td>\n",
       "      <td>4.060159</td>\n",
       "      <td>3.186776</td>\n",
       "      <td>0.479640</td>\n",
       "      <td>3.023999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.640351</td>\n",
       "      <td>6.402279</td>\n",
       "      <td>0.753167</td>\n",
       "      <td>4.271911</td>\n",
       "      <td>1.685490</td>\n",
       "      <td>24.056648</td>\n",
       "      <td>1.345958</td>\n",
       "      <td>3.380708</td>\n",
       "      <td>10.149232</td>\n",
       "      <td>0.879869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649055</td>\n",
       "      <td>4.634207</td>\n",
       "      <td>1.293985</td>\n",
       "      <td>1.923692</td>\n",
       "      <td>4.270373</td>\n",
       "      <td>0.514428</td>\n",
       "      <td>4.045384</td>\n",
       "      <td>3.160800</td>\n",
       "      <td>0.420076</td>\n",
       "      <td>2.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.633367</td>\n",
       "      <td>6.364065</td>\n",
       "      <td>0.746558</td>\n",
       "      <td>4.273916</td>\n",
       "      <td>1.675544</td>\n",
       "      <td>24.175880</td>\n",
       "      <td>1.307043</td>\n",
       "      <td>3.283266</td>\n",
       "      <td>10.381605</td>\n",
       "      <td>0.849728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639614</td>\n",
       "      <td>4.604102</td>\n",
       "      <td>1.244898</td>\n",
       "      <td>1.914263</td>\n",
       "      <td>4.237310</td>\n",
       "      <td>0.500878</td>\n",
       "      <td>3.985703</td>\n",
       "      <td>3.166997</td>\n",
       "      <td>0.407860</td>\n",
       "      <td>2.509081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.653618</td>\n",
       "      <td>6.446232</td>\n",
       "      <td>0.766516</td>\n",
       "      <td>4.284003</td>\n",
       "      <td>1.707747</td>\n",
       "      <td>23.937712</td>\n",
       "      <td>1.415982</td>\n",
       "      <td>3.405076</td>\n",
       "      <td>9.820030</td>\n",
       "      <td>0.918398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665805</td>\n",
       "      <td>4.616825</td>\n",
       "      <td>1.339860</td>\n",
       "      <td>1.941438</td>\n",
       "      <td>4.299609</td>\n",
       "      <td>0.550805</td>\n",
       "      <td>4.003161</td>\n",
       "      <td>3.196035</td>\n",
       "      <td>0.438132</td>\n",
       "      <td>2.742259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4          5         6   \\\n",
       "0   0.656015  6.453666  0.768478  4.208766  1.711249  23.937073  1.410413   \n",
       "1   0.672803  6.568323  0.782105  4.170029  1.731601  23.595064  1.468344   \n",
       "2   0.642758  6.388806  0.756561  4.143879  1.694888  24.085365  1.331089   \n",
       "3   0.662778  6.432861  0.772553  4.125156  1.721803  23.966825  1.420724   \n",
       "4   0.638319  6.400237  0.753536  4.225998  1.682347  24.098427  1.318871   \n",
       "5   0.654502  6.540128  0.766733  4.363331  1.703228  23.737345  1.421851   \n",
       "6   0.635506  6.352584  0.752524  4.157109  1.682888  24.222301  1.312239   \n",
       "7   0.641125  6.410493  0.755701  4.136579  1.688721  24.099165  1.318618   \n",
       "8   0.654554  6.413641  0.764487  4.146720  1.715249  24.026310  1.400277   \n",
       "9   0.655097  6.463675  0.765322  4.193246  1.714998  23.924068  1.404198   \n",
       "10  0.676916  6.612848  0.788424  4.206869  1.751563  23.389954  1.535797   \n",
       "11  0.640351  6.402279  0.753167  4.271911  1.685490  24.056648  1.345958   \n",
       "12  0.633367  6.364065  0.746558  4.273916  1.675544  24.175880  1.307043   \n",
       "13  0.653618  6.446232  0.766516  4.284003  1.707747  23.937712  1.415982   \n",
       "\n",
       "          7          8         9   ...        15        16        17  \\\n",
       "0   3.489094   9.807796  0.927695  ...  0.669512  4.619187  1.381890   \n",
       "1   3.591688   9.417401  0.964153  ...  0.687672  4.599662  1.422911   \n",
       "2   3.462164  10.115726  0.890888  ...  0.651254  4.589000  1.345221   \n",
       "3   3.472657   9.901963  0.918007  ...  0.676007  4.566734  1.372900   \n",
       "4   3.366527  10.279809  0.868069  ...  0.645860  4.581454  1.287504   \n",
       "5   3.334310   9.805918  0.912278  ...  0.666816  4.659933  1.319691   \n",
       "6   3.450688  10.193728  0.869164  ...  0.644318  4.544249  1.284395   \n",
       "7   3.484531  10.074389  0.886544  ...  0.648696  4.583266  1.338185   \n",
       "8   3.398870   9.919557  0.914965  ...  0.665617  4.563376  1.357731   \n",
       "9   3.403003   9.849689  0.911486  ...  0.665524  4.581716  1.336415   \n",
       "10  3.658931   9.125455  0.985704  ...  0.695497  4.562437  1.407352   \n",
       "11  3.380708  10.149232  0.879869  ...  0.649055  4.634207  1.293985   \n",
       "12  3.283266  10.381605  0.849728  ...  0.639614  4.604102  1.244898   \n",
       "13  3.405076   9.820030  0.918398  ...  0.665805  4.616825  1.339860   \n",
       "\n",
       "          18        19        20        21        22        23        24  \n",
       "0   1.947082  4.356538  0.539389  4.087608  3.165340  0.439703  2.732345  \n",
       "1   1.959408  4.461899  0.564665  4.121840  3.191759  0.462567  2.874877  \n",
       "2   1.900900  4.380582  0.492553  4.166725  3.182740  0.422401  2.609814  \n",
       "3   1.924317  4.390543  0.527891  4.145574  3.189193  0.445519  2.787483  \n",
       "4   1.909834  4.285573  0.501148  4.051194  3.168123  0.416203  2.543793  \n",
       "5   1.953964  4.238883  0.563509  3.934856  3.190670  0.439919  2.725107  \n",
       "6   1.890880  4.366358  0.496668  4.098042  3.160381  0.415725  2.561428  \n",
       "7   1.896849  4.400714  0.490130  4.172370  3.179897  0.420714  2.583388  \n",
       "8   1.911407  4.375976  0.519795  4.118711  3.223119  0.437713  2.732165  \n",
       "9   1.914054  4.369750  0.529535  4.066313  3.221978  0.438410  2.736626  \n",
       "10  1.943234  4.512201  0.609946  4.060159  3.186776  0.479640  3.023999  \n",
       "11  1.923692  4.270373  0.514428  4.045384  3.160800  0.420076  2.540500  \n",
       "12  1.914263  4.237310  0.500878  3.985703  3.166997  0.407860  2.509081  \n",
       "13  1.941438  4.299609  0.550805  4.003161  3.196035  0.438132  2.742259  \n",
       "\n",
       "[14 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pred.cpu().detach().numpy())\n",
    "df = df*100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./4.Result/0902_CGL2_SCT_reults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./4.Result/0902_CGL2_harmony_reults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
